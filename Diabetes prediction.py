# -*- coding: utf-8 -*-
"""FBA_Assignment_2020B5A42176H.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rR-ruZ7dM4kQOEcd6ezV4SNQ96R_Z7yH
"""

import pandas as pd
import numpy as np

"""# 1.) Dataset selection:

We select a dataset which gives information about pregnant woman and predect whether they have diabeties or not ( Target variable) based on features like Glucose, Blood Pressure, Skin thickness, INsulin, BMI, DiabetesPedegreeFunction, Age, Outcome
"""

## from google.colab import files
## uploaded = files.upload()
data = pd.read_csv("diabetes2_csv.csv")

"""# 2.) Data overview:"""

data.head()

data.shape

print("Nature of Variables:")
print("Quantitative Variables:")
print(data.select_dtypes(include=['int64', 'float64']).columns.tolist())
print("\nQualitative Variables:")
print(data.select_dtypes(include=['object']).columns.tolist())
print()

"""# 3.) Data Type identification:"""

variable_types = {}

for column in data.columns:
    dtype = data[column].dtype
    if dtype == 'int64' or dtype == 'float64':
        variable_types[column] = 'Numerical'
    elif dtype == 'object':
        unique_values = data[column].nunique()
        if unique_values <= 10:
            variable_types[column] = 'Categorical'
        else:
            variable_types[column] = 'Text'
    else:
        variable_types[column] = 'Other'

print("Classification of Variables into Respective Data Types:")
for variable, data_type in variable_types.items():
    print(f"{variable}: {data_type}")

"""Missing Value Assessment:

# 4 &5 ) Missing values and Data cleaning:

As we can see out of 729 rows, there are no null values in any rows. Hence, cleaning of data is not necessary in this case
"""

data.info()

"""# 6.) Outlier Detection:"""

data.iloc[:-1].replace(0, np.nan, inplace=True)

missing_values = data.isnull().sum()

missing_percentage = (missing_values / len(data)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

missing_summary = missing_summary.sort_values(by='Percentage', ascending=False)

print("Summary of Missingness Pattern:")
print(missing_summary)

data.drop_duplicates(inplace=True)

"""# 7.) Handling outliers: (By impution)


"""

data.mean()

data = data.fillna(data.mean())

data.isnull().sum()

from sklearn.ensemble import IsolationForest


model = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed

model.fit(data)

outliers = model.predict(data)

outlier_indices = data.index[outliers == -1]

print("Indices of Outlier Rows:")
print(outlier_indices)

data.dtypes

data = data.drop(outlier_indices) #outlier removal, we have done so as they hamper with the predictions

#Normalized all columns except the Outcomes.
def zscore_normalization(x):
    if x.name != data.columns[-1]:
        return (x - x.mean()) / x.std()
    else:
        return x  # Return the original column if it's the last one

data = data.apply(zscore_normalization)


data.head()

"""# 8.) Data Transformations:

Logistic regression done in the 20th qn

# 9.) We use the code to encode categorical variables

WE can see that there are no categorical coloumns in out dataset, hence encoding is not done
"""

categorical_columns = data.select_dtypes(include=['object']).columns

data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

#As our dataset has no categorical columns this is reedundant, and we do not use any encoding choices

data.describe()

"""# 10.) We do explorartory data analysis on the given dataset:"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# Summary statistics
print("Summary Statistics:")
print(data.describe())

print("\nCorrelation Matrix:")
corr = data.corr()
print(corr)

plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix Heatmap")
plt.show()

sns.pairplot(data, diag_kind='kde')
plt.suptitle("Pairplot of the Iris Dataset", y=1.02)
plt.show()

"""# 11.) Visual EDA distribution:

(for pregnancies, glucose and Blood pressure     Variables )
"""

# Select key variables for exploration (replace with your variables)
key_variables = ['Pregnancies', 'Glucose', 'BloodPressure']

fig, axes = plt.subplots(nrows=len(key_variables), ncols=2, figsize=(12, len(key_variables) * 5))

# Create histograms and box plots for each key variable
for i, variable in enumerate(key_variables):
    # Histogram
    sns.histplot(data[variable], ax=axes[i, 0], kde=True, color='skyblue', bins=20)
    axes[i, 0].set_title(f'Histogram of {variable}')
    axes[i, 0].set_xlabel(variable)
    axes[i, 0].set_ylabel('Frequency')

    # Box plot
    sns.boxplot(x=data[variable], ax=axes[i, 1], color='lightgreen')
    axes[i, 1].set_title(f'Box Plot of {variable}')
    axes[i, 1].set_xlabel(variable)

# Adjust layout
plt.tight_layout()
plt.show()

"""# 12.)EDA relationships using Heatmaps and scatter plots:"""

import seaborn as sns
import matplotlib.pyplot as plt

# Select key variables for investigation (replace with your variables)
variable1 = 'Pregnancies'
variable2 = 'Glucose'
target_variable = 'BloodPressure'

# Scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=variable1, y=variable2, data=data, hue=target_variable, palette='viridis')
plt.title(f'Scatter Plot of {variable1} vs {variable2} (Colored by {target_variable})')
plt.xlabel(variable1)
plt.ylabel(variable2)
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
corr = data.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title(f'Correlation Heatmap between {variable1} and {variable2}')
plt.show()

"""# 13.) Identify and Interpret Key Relationships:

As we can see we have a seemingly high correlation factors mainly between

   1.) Pregnancies and Age

   2.)BMI and SkinThickness

->If we want to we can remove a variable in each but in outr case, this leads to loss of data, so we would not do that

Potential impact on business problems:

High correlation between variables can lead to multicollinearity, making it difficult to separate their individual effects. Interpretation becomes ambiguous, and including redundant variables may cause overfitting.

# 14.) Descriptive statistics summary:

glucose levels and BMI are notably higher in pregnant women with diabetes compared to those without. This highlights the significance of these factors in predicting diabetes risk, emphasizing the importance of monitoring glucose levels and managing BMI to mitigate diabetes risks in pregnant women.

# 15.) Statistical Questions:

Based on the provided dataset concerning pregnant women and predicting whether they have diabetes or not, here are two statistical questions that could further inform the business problem:

Q1.) Is there a significant difference in the mean values of glucose levels between pregnant women with and without diabetes?

Q2.) What is the association between body mass index (BMI) and the likelihood of having diabetes among pregnant women?

# 16.) Conducting a T-Test:
"""

from scipy.stats import ttest_ind

# Generate some sample data for two groups
# Replace this with your actual dataset or sample data
group1 = np.random.normal(loc=10, scale=2, size=100)
group2 = np.random.normal(loc=12, scale=2, size=100)

# Perform the two-sample t-test
t_statistic, p_value = ttest_ind(group1, group2)

# Significance level (alpha)
alpha = 0.05

# Print the results
print("Test Statistic (t-value):", t_statistic)
print("p-value:", p_value)

# Compare p-value with significance level to make a decision
if p_value < alpha:
    print("Reject the null hypothesis. There is a significant difference in means.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in means.")

data.describe()

""" Data split: (train and test)"""

# Iterate through each column in the DataFrame
for column in data.columns:
    # Check if the column contains non-numeric data
    if data[column].dtype == 'object':
        try:
            # Attempt to convert the column to numeric
            data[column] = pd.to_numeric(data[column])
        except ValueError:
            # Handle columns with non-convertible data (e.g., text)
            print(f"Column '{column}' contains non-numeric data and cannot be converted.")
            # You might want to decide how to handle these columns, whether to drop or encode them

# Now, all columns in the DataFrame should be numerical,
# or they will be flagged for manual handling in the exception block above

data.dtypes

from sklearn.model_selection import train_test_split


X = data.drop('Outcome', axis=1)  # Features
Y = data['Outcome']  # Target

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets
print("X_train shape:", X_train.shape)
print("Y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("Y_test shape:", y_test.shape)

"""# 17.)Inferential Statistics Conceptual Application

The possibility of the woman being diabetic or not is mainly based on glucose and pregnency as they have the highest relation with the outcome vaiable
BUt other variables though play an importhant role as they affect other variables and indirectly affect the outcome

# 18.) Predictive modelling:

Based on the datset we have and the problem ststement, I would like to predict wether the pregnent women is diabetic or not based on all other feature variables

# 19.) Feature selection rationalle:

->WE conduct exploratory data analysis to identify correlations, importance, and relevance to the outcome variable

->We could analyze correlations with the outcome variable, evaluate feature importance using techniques like tree-based models, and prioritize features relevant to the outcome variable based on domain knowledge and business understanding.

# 20.) Feature Selection based on

# a)F score
"""

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest, SelectPercentile

sel = f_classif(X_train, y_train)
sel

p_values = pd.Series(sel[1])
p_values.index = X_train.columns
p_values.sort_values(ascending = True, inplace = True)

p_values.plot.bar(figsize = (16, 5))

p_values = p_values[p_values<0.05]

X_train_p = X_train[p_values.index]
X_test_p = X_test[p_values.index]

X_test_p.head()

new_cols = X_train.columns
new_cols

"""# b.) PCA"""

from sklearn.decomposition import PCA

X_train.shape

pca = PCA(n_components=8, random_state=42)
pca.fit(X_train)

pca.explained_variance_

pca.explained_variance_ratio_

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
X_train_pca.shape, X_test_pca.shape

pd.DataFrame(X_train_pca).corr()

X_train.corr()

X_train_pca

"""# c.) Logistic regression

"""

t = np.linspace(-100, 100, 1000)
sig = 1 / (1 + np.exp(-t))
plt.figure(figsize=(9, 3))
plt.plot(t, sig, "b-", linewidth=2, label=r"$\sigma(t) = \frac{1}{1 + e^{-t}}$")
plt.xlabel("t")
plt.legend(loc="upper left", fontsize=20)
plt.axis([-10, 10, -0.1, 1.1])
plt.show()

from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, f1_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve, ConfusionMatrixDisplay,RocCurveDisplay


model = LogisticRegression()
model.fit(X_train,y_train)

"""### 3 Feature split :"""

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression


# 1. Feature Importance from Tree-Based Models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
feature_importances = rf.feature_importances_

# Select top-n features based on feature importances
top_n = 2  # Example: select top 2 features
selected_features_tree_based = sorted(range(len(feature_importances)), key=lambda i: feature_importances[i], reverse=True)[:top_n]

# 2. SelectFromModel with Decision Tree Classifier
dt = DecisionTreeClassifier(random_state=42)
selector = SelectFromModel(dt, max_features=top_n)
selector.fit(X_train, y_train)
selected_features_decision_tree = selector.get_support(indices=True)

# 3. Univariate Feature Selection with SelectKBest
skb = SelectKBest(score_func=f_classif, k=top_n)
X_train_skb = skb.fit_transform(X_train, y_train)
selected_features_skb = skb.get_support(indices=True)

# Print selected features from each technique
print("Selected Features from Feature Importance (Tree-Based Models):", selected_features_tree_based)
print("Selected Features from SelectFromModel (Decision Tree Classifier):", selected_features_decision_tree)
print("Selected Features from Univariate Feature Selection (SelectKBest):", selected_features_skb)

"""# 21.) KNN predictive modelling technique:"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialize the KNN classifier with a specified number of neighbors (e.g., k=5)
k = 10
knn_classifier = KNeighborsClassifier(n_neighbors=k)

# Train the KNN classifier on the training data
knn_classifier.fit(X_train, y_train)

# Predict the target variable for the test set
Y_pred = knn_classifier.predict(X_test)

# Evaluate the performance of the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, Y_pred))
print("\nClassification Report:")
print(classification_report(y_test, Y_pred))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score


# Initialize the DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""# 23.) Presentation of findings:

We predict diabetes in pregnant women based on features like glucose, blood pressure, BMI, etc. Key findings include identifying glucose levels and BMI as significant predictors, allowing targeted interventions and risk management strategies to mitigate diabetes risks in this vulnerable population.

# 24.)Interpretation of Predictive Model Output

After KNN modeling, interpreting the model's output involves assessing the predicted probabilities or class labels for each observation. By analyzing the probabilities or classifications, we can identify pregnant women at higher risk of diabetes based on their features.

# 25.) Reflect on Data Governance and Ethics:

The diabetes outcome dataset likely contains sensitive health information, requiring strict adherence to data privacy regulations. To address this, data should be anonymized to protect patient identities. Access controls must be implemented to restrict data access to authorized personnel only. Informed consent from participants should be obtained, and ongoing compliance with regulations such as HIPAA ensured through regular audits.

# 26.)Future step for analysis:

Additional data that could enhance insights and recommendations include dietary habits, physical activity levels, family medical history of diabetes, gestational age at diagnosis, and previous pregnancy outcomes. This data would provide a more comprehensive understanding of risk factors and help tailor interventions for diabetes prevention in pregnant women.

# 27.) Conclusion

The project identified glucose levels and BMI as significant predictors of diabetes in pregnant women, highlighting the importance of monitoring these factors for early detection and intervention. Limitations include the absence of dietary and genetic data, which could enhance predictive accuracy. Future improvements involve incorporating additional features and leveraging advanced modeling techniques for more precise risk assessment and personalized interventions.

# 28.) Appendix

Variables:
    Glucose
    
    1.)Blood Pressure
    
    2.)Skin thickness
    
    3.)INsulin
    
    4.)BMI
    
    5.)DiabetesPedegreeFunction
    
    6.)Age
    
    7)Outcome

# 29.) Data Dictionary:

->Glucose: The glucose concentration measured in milligrams per deciliter (mg/dL) in the blood.

->BloodPressure: The diastolic blood pressure measured in millimeters of mercury (mmHg).

->SkinThickness: The thickness of the skinfold at the triceps site, measured in millimeters.

->Insulin: The insulin concentration measured in milli-international units per liter (mu U/ml).

_>BMI: The Body Mass Index (BMI) calculated as weight in kilograms divided by the square of height in meters
(kg/m^2).

->DiabetesPedigreeFunction: A function that scores the likelihood of diabetes based on family history.

->Age: The age of the individual in years.

->Outcome: The target variable indicating the presence (1) or absence (0) of diabetes.

Each variable plays a role in predicting the likelihood of diabetes in pregnant women. Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, and Age are the predictor variables, while Outcome is the target variable used for classification analysis.

# 30,) Addictional resources used:
    
-> Dataset : C
Kaggle

-> Python libraries for EDA, Modelling etc

-> codes form scikit learn website
"""

